# Example involving the man-hours worked per month and the total number of items
# processed at a sample of 22 US Naval sites.

# Read in the data:

navymh <- read.table("F:/STAT/STAT2008/Datasets (text)/navy.txt", header=T)
navymh
names(navymh)

# Attach and plot the data:

attach(navymh)
plot(manhours, items)

# Start with an initial plausible model - in this case, a SLR model:

navymh.lm <- lm(items ~ manhours)

# Add the model to the plot we created earlier:

abline(navymh.lm$coef)

# Looks like a good fit, but let's have a closer look at the main residual plot:

plot(fitted(navymh.lm), residuals(navymh.lm))

# Add the model to this plot:

abline(0,0,lty=2)

# Not exactly the random scatter around the model, we were hoping for, so let's 
# identify the points and see if we can see what is going on:

identify(fitted(navymh.lm), residuals(navymh.lm))

# The data were roughly sorted by size, i.e. smaller establishments (in terms of 
# man-hours) came first. This "clustering" effect in the data may be a result of
# different rounding being used for different size establishments, i.e. records 
# are more exact for the smaller establshments, but are more likely to be rounded
# (i.e. have greater error) for the larger establishments.

# We also need to check the Normal Q-Q plot, which, when we use the default plot
# function, uses standardised residuals (more on these later in the course):

plot(navymh.lm, which=2)

# This plot was generated using the qqnorm(function):

help(qqnorm)

# Not the interesting choice of a "theoretical line" in the help file description.
# We could also have used a "Y=X" line (which in this instance, isn't really different):
 
abline(0,1,lty=3)

# The plot.lm() function has chosen to highlight the 3 smallest residuals (14, 19 and 21)
# as potential problems, even though their standardised residual values are within plus 
# or minus 2! If we look at the help file for plot.lm(), we find that it by default it
# will always label the three observations with the most extreme residual values. 
# We will discuss problems with individual observations later in the course, but at this
# stage we can easily generate a couple of plots to check there really is no problem.

# A bar chart (plot) of the leverage values - the diagonal elements from the Hat matrix:

lm.influence(navymh.lm)
barplot(lm.influence(navymh.lm)$hat)

# Another measure we will look at later in the course are Cook's distances, which is an
# attempt to measure both influence in the fitting of the model (leverage) as well as
# measuring whether or not an observation is an outlier.

plot(navymh.lm, which=4)

# For both of these plots, it is best to judge whether a point is extreme compared to 
# the other observations, i.e. make a relative judgement. Whereas a different
# small group of points appears to behave differently in both points, the default 
# levarage plot produced by plot.lm(), which includes arbitrary limits for Cook's 
# distance values, suggests that these are a not really a problem as no observation 
# lies outside these arbitrary limits:

plot(navymh.lm, which=4)

# As there are no obvious violations of the assumptions specific to a SLR model,
# we can treat the model as an appropriate choice and proceeed to assess and 
# interpret other aspects of the model:

anova(navymh.lm)

# The F-test which, for a SLR model, can be viewed as a test of the both whether
# the variance explained by the model is large relative to the residual variance 
# AND a test of whether the slope coefficient involving manhours is significantly
# different from zero, suggests that a large part of the variation in the response
# variable (items) is explained by the model involving manhours and that there is
# a significant relationship between items and manhours.

summary(navymh.lm)

# The t-test on manhours confirms this relationship (for SLR, it is a equivalent 
# test to the overall F test). Note that the t-test on the intercept suggest that
# it is not significantly different from 0. This appears reasonable, we have data 
# for relatively small establishments and having 0 available manhours should
# result in 0 items being produced. Should we delete the intercept? We can try 
# and see what happens:

navymh.lm_noi <- lm(items ~ manhours - 1)

# This has resulted in some changes to the summary output, but no changes in the
# values of the various tests:

anova(navymh.lm)
anova(navymh.lm_noi)

summary(navymh.lm)
summary(navymh.lm_noi)

# But what about the residual plots?

plot(navymh.lm_noi)

# Again, almost, but not quite identical - even though the R-square appears to 
# have increased slightly, there are now points just outside the arbitrary limits
# on the default leverage plot. In general, we will always fit an intercept to
# allow maximum flexibility in how the model approximates the data, unless there 
# is a really strong reason (suggested by the underlying science) to propose a
# particular restricted form of the model.

